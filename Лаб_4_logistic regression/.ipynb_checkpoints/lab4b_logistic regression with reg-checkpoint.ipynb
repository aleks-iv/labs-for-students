{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логістична регресія з регуляризацією"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaddata(file, delimeter):\n",
    "    data = np.loadtxt(file, delimiter=delimeter)\n",
    "    print('Dimensions: ', data.shape)\n",
    "    print(data[1:6,:])\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(data, label_x, label_y, label_pos, label_neg, axes=None):\n",
    "    # Get indexes for class 0 and class 1\n",
    "    neg = data[:, 2] == 0\n",
    "    pos = data[:, 2] == 1\n",
    "    \n",
    "    # If no specific axes object has been passed, get the current axes.\n",
    "    if axes == None:\n",
    "        axes = plt.gca()\n",
    "    axes.scatter(data[pos][:,0], data[pos][:,1], marker='x', c='green', s=60, linewidth=2, label=label_pos)\n",
    "    axes.scatter(data[neg][:,0], data[neg][:,1], c='darkorange', s=60, label=label_neg)\n",
    "    axes.set_xlabel(label_x)\n",
    "    axes.set_ylabel(label_y)\n",
    "    axes.legend(frameon= True, fancybox = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Theta, X, threshold=0.5):\n",
    "    p = sigmoid(np.dot(X, Theta)) >= threshold\n",
    "    return(p.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = loaddata('data2.txt', ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.c_[data2[:,2]]\n",
    "X_raw = data2[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData(data2, 'Microchip Test 1', 'Microchip Test 2', 'y = 1', 'y = 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this function inserts a column with 'ones' in the design matrix for the intercept.\n",
    "poly = PolynomialFeatures(6)\n",
    "X = poly.fit_transform(data2[:,0:2])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функція варотості логістичної регресії з регуляризацією\n",
    "$$ J(\\Theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\big[-y^{(i)}\\, log\\,( h_\\Theta\\,(x^{(i)}))-(1-y^{(i)})\\,log\\,(1-h_\\theta(x^{(i)}))\\big] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}$$\n",
    "Функція вартості у векторизованому вигляді\n",
    "$$ J(\\Theta) = \\frac{1}{m}\\big(-y^T\\,log\\,(\\sigma(X\\Theta))-(1-y)^T(\\,log\\,(1-\\sigma(X\\Theta))\\big) + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunctionReg(Theta, X, y, lmbd):\n",
    "    m = y.size\n",
    "    \n",
    "    a = sigmoid(np.dot(X, Theta))\n",
    "    \n",
    "    J = (1/m)*(np.dot(-y.T, np.log(a)) - np.dot((1 - y).T, np.log(1 - a))) + (lmbd/(2*m))*np.sum(np.square(Theta[1:])) \n",
    "    \n",
    "    if np.isnan(J[0]):\n",
    "        return(np.inf)\n",
    "    return(J[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градієнт функції з регуляризацією втрат у векторизованому вигляді\n",
    "$$ \\nabla J(\\Theta) = \\frac{1}{m} X^T(\\sigma(X\\Theta)-y) + \\frac{\\lambda}{m}\\theta_{j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientReg(Theta, X, y, lmbd):\n",
    "    m = y.size\n",
    "    \n",
    "    a = sigmoid(np.dot(X, Theta.reshape(-1,1)))\n",
    "      \n",
    "    grad = (1/m)*X.T.dot(a-y) + (lmbd/m)*np.r_[[[0]], Theta[1:].reshape(-1,1)]\n",
    "        \n",
    "    return(grad.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_Theta = np.zeros(X.shape[1]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costFunctionReg(init_Theta, X, y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3, sharey = True, figsize=(17,5))\n",
    "\n",
    "# Decision boundaries\n",
    "# Lambda = 0 : No regularization --> too flexible, overfitting the training data\n",
    "# Lambda = 1 : Looks about right\n",
    "# Lambda = 100 : Too much regularization --> high bias\n",
    "\n",
    "for i, lmbd in enumerate([0, 1, 100]):\n",
    "    # Optimize costFunctionReg\n",
    "    res = minimize(costFunctionReg, init_Theta, args=(X, y, lmbd), method=None, jac=gradientReg, options={'maxiter':3000})\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = 100*sum(predict(res.x, X) == y.ravel())/y.size    \n",
    "\n",
    "    # Scatter plot of X,y\n",
    "    plotData(data2, 'Microchip Test 1', 'Microchip Test 2', 'y = 1', 'y = 0', axes.flatten()[i])\n",
    "    \n",
    "    # Plot decisionboundary\n",
    "    x1_min, x1_max = X_raw[:,0].min(), X_raw[:,0].max(),\n",
    "    x2_min, x2_max = X_raw[:,1].min(), X_raw[:,1].max(),\n",
    "    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "    a = sigmoid(poly.fit_transform(np.c_[xx1.ravel(), xx2.ravel()]).dot(res.x))\n",
    "    a = a.reshape(xx1.shape)\n",
    "    axes.flatten()[i].contour(xx1, xx2, a, [0.5], linewidths=1, colors='blue');       \n",
    "    axes.flatten()[i].set_title('Train accuracy {}% with $\\lambda$ = {}'.format(np.round(accuracy, decimals=2), lmbd))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
